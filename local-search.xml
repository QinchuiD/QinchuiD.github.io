<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>CS224n Assignment1</title>
    <link href="/2025/12/23/1/"/>
    <url>/2025/12/23/1/</url>
    
    <content type="html"><![CDATA[<hr><p>通过svd实现词向量的生成</p><h2 id="问题1-1-实现distinct-words"><a href="#问题1-1-实现distinct-words" class="headerlink" title="问题1.1:实现distinct_words"></a>问题1.1:实现distinct_words</h2><p>编写一个方法计算语料库中出现的不同单词:<br>把语料库中的每个词存入列表中，再把列表转为集合，就去除了重复的词，集合的长度就是不同单词的数量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">corpus_words=[word <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> corpus <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentence]<br>corpus_words=<span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">set</span>(corpus_words))    n_corpus_words=<span class="hljs-built_in">len</span>(corpus_words)<br></code></pre></td></tr></table></figure></p><h2 id="问题-1-2-实现compute-co-occurrence-matrix"><a href="#问题-1-2-实现compute-co-occurrence-matrix" class="headerlink" title="问题 1.2:实现compute_co_occurrence_matrix"></a>问题 1.2:实现compute_co_occurrence_matrix</h2><p>创建共现矩阵，共现矩阵M统计了词语之间相邻出现的频率，对于一个中心词w,记录围绕w以n为窗口大小的单词。$M_{i,j}$统计了$w_i$和$w_j$在相邻出现的次数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">M = np.zeros((n_words, n_words), dtype=np.float64)<br>word2ind = &#123;word : i <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(words)&#125;<br><span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> corpus:<br>    n=<span class="hljs-built_in">len</span>(sentence)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i-window_size,i+window_size+<span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">if</span> j&lt;<span class="hljs-number">0</span> <span class="hljs-keyword">or</span> j&gt;=n <span class="hljs-keyword">or</span> j==i:<br>                    <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">else</span>:<br>    M[word2ind[sentence[i]]][word2ind[sentence[j]]]+=<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><br>初始化矩阵M遍历统计每个单词作为中心词的单词出现情况</p><h2 id="问题-1-3：实现-reduce-to-k-dim"><a href="#问题-1-3：实现-reduce-to-k-dim" class="headerlink" title="问题 1.3：实现 reduce_to_k_dim"></a>问题 1.3：实现 reduce_to_k_dim</h2><p>构建一个对矩阵进行降维生成k维词向量的方式，使用[[奇异值分解SVD]]提取前k个分量，生成一个新的k维词向量矩阵<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">svd = TruncatedSVD(n_components=k, n_iter=n_iters)<br>M_reduced = svd.fit_transform(M)<br></code></pre></td></tr></table></figure><br>通过封装好的函数TruncatedSVD得到一个降至k维，迭代n_iters次生成的svd方式，并用于处理M</p><h2 id="Question-1-4-Implement-plot-embeddings"><a href="#Question-1-4-Implement-plot-embeddings" class="headerlink" title="Question 1.4: Implement plot_embeddings"></a>Question 1.4: Implement plot_embeddings</h2><p>绘制词向量图像，因为前面将词向量转化成了2维向量，所以可以在平面上画出<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>    [x, y] = M_reduced[word2ind[word]]<br>    plt.scatter(x, y, marker=<span class="hljs-string">&#x27;x&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br>    plt.text(x, y, word, fontsize=<span class="hljs-number">9</span>)<br>    plt.show()<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
      <category>自然语言处理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>CS224n</tag>
      
      <tag>词向量</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CS224n 学习笔记 (一)：词向量</title>
    <link href="/2025/12/22/lecture1-Word2vec/"/>
    <url>/2025/12/22/lecture1-Word2vec/</url>
    
    <content type="html"><![CDATA[<h1 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h1><ul><li><strong>分布式假设</strong>：同境词义近。</li><li><strong>词嵌入</strong>：将离散符号转化为高维连续向量，解决One-hot 的稀疏性与维度灾难。</li></ul><hr><p>语言是促进人类发展的重要工具，我们希望计算机也能理解人类的语言。<br>同时，每个词的意思是在具体的语境中体现的，一个词语可能也有不同的意思，所以我们希望计算机也能理解这一点。<br>显然不能采用孤立的符号(one-hot编码)来表示单词，因为体现不出相似性且维度较大，所以使用紧凑的词向量(word vector)来表示每一个单词，通过向量间点积的大小来表示两个单词的相似程度。<br>相当于我们把每个词嵌入到高维的空间中，空间中的距离远近表示相似程度的大小</p><hr><p>那么我们该怎么把单词嵌入到合适的位置呢？</p><h2 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h2><p>在文本中，如果两个词相邻出现，那么可以判定两个词逻辑上是相邻的，例如”the”和“student”常常相邻出现。<br>word2vec则运用了这一特点，通过计算与相邻词的相似程度去改变词向量的值。   </p><h3 id="Skip-gram-模型推导"><a href="#Skip-gram-模型推导" class="headerlink" title="Skip-gram 模型推导"></a>Skip-gram 模型推导</h3><script type="math/tex; mode=display">\text{Likelihood} = L(\theta) = \prod_{t=1}^{T} \prod_{-m \leq j \leq m \atop j \neq 0} P(w_{t+j} \mid w_t; \theta)</script><p>使用如上式子表示相似程度，我们希望使这个值最大，因为我们往往是求让某个函数最小，所以取负数来把问题转化，又因为算积比较慢，所以取对数将问题转化为求和。    </p><script type="math/tex; mode=display">J(\theta) = -\frac{1}{T}\log L(\theta) = -\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq j \leq m \atop j \neq 0} \log P(w_{t+j} \mid w_t; \theta)</script><p>这个概率表示以$w<em>t$为中心词时，$w</em>{t+j}$ 出现的概率，怎么计算这个概率呢？<br>规定每个词有两个向量，一个作为中心词是使用(v),一个作为相邻词时使用(μ)，使用softmax函数，将中心词与相邻词向量的点积与所有词和中心词点积之和的商作为概率。</p><script type="math/tex; mode=display">P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V} \exp(u_w^T v_c)}</script><p>在使用梯度下降时，我们分别要改变μ和v，使两个单词间距离缩小。</p><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>word2web实际上是通过相邻词来确定自身位置。<br>在向量空间中，将观察到的“中心词-背景词”对相互拉近。</p>]]></content>
    
    
    <categories>
      
      <category>计算机科学</category>
      
      <category>自然语言处理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>CS224n</tag>
      
      <tag>词向量</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>123</title>
    <link href="/2025/12/17/123/"/>
    <url>/2025/12/17/123/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
